#!/bin/env python3
import argparse
from benchgo.benchgo_trino import *

if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Benchmark control script"
    )
    parser.add_argument("--name", default="benchgo", help="name this run something - will show up in output directory")
    parser.add_argument("--benchmark", help="benchmark task to run (tpcds, tpcds_s, insert, update/delete)")

    parser.add_argument("--engine", help="target engine (trino, starburst, spark, dremio, vdb-sdk)")
    
    # Prometheus options
    parser.add_argument("--prometheus-host", help="URL for prometheus host (e.g.: http://myhost:3000)")
    parser.add_argument("--trino-prometheus-job", default="trino_1", help="Prometheus job name containing the node_exporter data for the Trino cluster")
    parser.add_argument("--cnode-prometheus-job", default="vast_cnodes", help="Prometheus job name containing the node_exporter data for the VAST CNode cluster")
    parser.add_argument("--sleep-between-queries", default=5, help="gap in seconds between queries to avoid overlap in stats collection")

    # Trino options
    parser.add_argument("--trino-coordinator", help="Trino coordinator URI (e.g.: http://trino_coord:8080)")
    parser.add_argument("--trino-password", default=None, help="Trino password")
    parser.add_argument("--trino-user", default='admin', help="Trino username")
    parser.add_argument("--trino-catalog", help="catalog housing target tpcds database")
    parser.add_argument("--trino-schema", help="schema housing target tpcds database")

    # Spark options
    #parser.add_argument("--spark-master", help="Spark master URI (e.g.: spark://spark_master:7077)")
    #parser.add_argument("--spark-executor-mem", default="32g", help="spark executor memory")
    #parser.add_argument("--spark-database-path", default="spark_default", help="full path to target database")

    # Dremio options
    pass

    # SDK options
    pass

    # TPC-DS options
    parser.add_argument("--tpcds-scale", help="TPC-DS scale factor sf1000, sf10000, sf100000; WARNING: this option selects the queries to use, not the data set")


    # TPC-DS_S options
    parser.add_argument("--tpcds-s-scale", help="TPC-DS scale factor sf1000, sf10000, sf100000; WARNING: this option selects the queries to use, not the data set")

    # Insert options
    parser.add_argument("--agent-nodes", help="comma-separated list of nodes to run on")
    parser.add_argument("--agent-procs", help="num procs to run per node")
    parser.add_argument("--insert-scale", help="rows to write (e.g.: 1000000)")
    parser.add_argument("--insert-batch", help="batch size for insertions")

    # Update/delete options
    parser.add_argument("--update-del-tests-sf", default="sf1", help="scale factor for tests (sf1,sf10,sf100,sf1000 or sf10000)")
    parser.add_argument("--update-del-table", default=None, help="full path of table to run tests on (only works on tables prepared with mkdata utility)")
    parser.add_argument("--merge-from-table", default=None, help="table (full path), to use as source for merge testing")

    # Spark-related
    parser.add_argument("--gen-spark-config", action="store_true", help="generate template configuration file for use with \"benchgo_spark\"")

    args = parser.parse_args()

    if args.gen_spark_config:
        from benchgo.spark_config_template import *
        try:
            with open(CONFIG_OUT_LOCATION, "x") as fh:
                fh.write(CONFIG_TEMPLATE)
        except FileExistsError:
            print("'{}' already exists, move or remove and try again".format(CONFIG_OUT_LOCATION))
    else:
        run_trino(args)